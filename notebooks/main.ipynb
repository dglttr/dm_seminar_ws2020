{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.regressionplots import abline_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "from minisom import MiniSom as SOM\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = r\"C:\\Users\\Daniel\\Documents\\Studium\\7. Semester (WS 2020.21)\\Seminar Data Mining in der Produktion\\Gruppenarbeit\\Data\"\n",
    "NORMALIZATION_NORM = \"l2\"\n",
    "TEST_TRAIN_SPLIT = 0.1\n",
    "RANDOM_SEED = 1     # for reproducibility\n",
    "WINDOW = 300    # rolling window to smoothen plots\n",
    "\n",
    "EXPERIMENT_NAMES = ['C11', 'C13', 'C14', 'C15', 'C16', 'C7', 'C8', 'C9']\n",
    "\n",
    "# Components\n",
    "COMPONENTS = [\n",
    "    ['A_1', 'A_2', 'A_3', 'A_4', 'A_5'],\n",
    "    ['B_1', 'B_2', 'B_3', 'B_4', 'B_5'],\n",
    "    ['C_1', 'C_2', 'C_3', 'C_4', 'C_5'],\n",
    "    ['L_1', 'L_2'],\n",
    "    ['L_3', 'L_6'],\n",
    "    ['L_4', 'L_5'],\n",
    "    ['L_7', 'L_8'],\n",
    "    ['L_9', 'L_10']\n",
    "]\n",
    "\n",
    "L_cols = ['L_1', 'L_2', 'L_3', 'L_4', 'L_5', 'L_6', 'L_7', 'L_8', 'L_9', 'L_10']\n",
    "A_cols = ['A_1', 'A_2', 'A_3', 'A_4', 'A_5']\n",
    "B_cols = ['B_1', 'B_2', 'B_3', 'B_4', 'B_5']\n",
    "C_cols = ['C_1', 'C_2', 'C_3', 'C_4', 'C_5']\n",
    "column_groups = [L_cols, A_cols, B_cols, C_cols]\n",
    "\n",
    "COLORMAP = {\n",
    "    \"L_2\": \"b\",\n",
    "    \"L_6\": \"g\",\n",
    "    \"L_8\": \"r\",\n",
    "    \"L_10\": \"c\",\n",
    "    \"A_5\": \"m\",\n",
    "    \"B_4\": \"y\",\n",
    "    \"B_5\": \"k\",\n",
    "    \"C_5\": \"tab:orange\"\n",
    "}\n",
    "\n",
    "print(\"Constants set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory: str):\n",
    "    _filepaths = [directory + \"/\" + filename for filename in os.listdir(directory)]\n",
    "\n",
    "    files = [pd.read_csv(path) for path in _filepaths]\n",
    "\n",
    "    # Merge C13-1 and C13-2 as well as C7-1 and C7-2\n",
    "    c13 = pd.concat([files[1], files[2]])\n",
    "    c7 = pd.concat([files[6], files[7]])\n",
    "\n",
    "    files[1] = c13\n",
    "    files[6] = c7\n",
    "\n",
    "    files.pop(2)\n",
    "    files.pop(7)\n",
    "\n",
    "    # Drop Timestamp column\n",
    "    files = [df.drop(\"Timestamp\", axis=1) for df in files]\n",
    "\n",
    "    # Handle NaN\n",
    "    files = [df.dropna() for df in files]\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def normalize_files(files: List[pd.DataFrame], normalization_norm: str):\n",
    "    column_names = files[0].columns\n",
    "    files = [sklearn.preprocessing.normalize(df, norm=normalization_norm) for df in files]\n",
    "    files = [pd.DataFrame(array, columns=column_names) for array in files]\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def test_train_split(files: List[pd.DataFrame], split: float) -> Tuple[List[pd.DataFrame], List[pd.DataFrame]]:\n",
    "    # Split train and test data -> first x% taken (not randomly)\n",
    "    def split_df_not_randomly(df, split: float):\n",
    "        split_index = int(len(df) * split)\n",
    "        return df[:split_index], df[split_index:]\n",
    "\n",
    "    train_files, test_files = zip(*[split_df_not_randomly(df, split) for df in files])\n",
    "\n",
    "    return train_files, test_files\n",
    "\n",
    "\n",
    "def plot_all_experiments(datasets: list, experiment_names: list, test_train_split: float = None,\n",
    "                         savefig: str = \"C:/Users/Daniel/Desktop/plot.png\", ols_line: bool = False) -> None:\n",
    "    fig, ax = plt.subplots(8, 1)\n",
    "    fig.set_size_inches(6, 24)\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "            axis = ax[i]\n",
    "            axis.scatter(x=dataset.index, y=dataset[0], s=1)\n",
    "            axis.set_title(f\"Experiment No. {experiment_names[i]}\")\n",
    "\n",
    "            if test_train_split is not None:    # plot vertical line\n",
    "                test_train_split_index = int(len(dataset) * test_train_split)\n",
    "                axis.axvline(x=test_train_split_index, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "            if ols_line:\n",
    "                x = sm.add_constant(dataset.index)\n",
    "                y = dataset[0]\n",
    "                abline_plot(model_results=sm.OLS(y, x).fit(), ax=axis, color=\"black\", linewidth=1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if savefig:\n",
    "        plt.savefig(savefig)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Utils imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "files = load_data(directory=DATA_DIRECTORY)\n",
    "\n",
    "# Data normalization\n",
    "files = normalize_files(files, normalization_norm=NORMALIZATION_NORM)\n",
    "\n",
    "# Feature selection\n",
    "cols_to_include = [\"L_2\", \"L_6\", \"L_8\", \"L_10\", \"A_5\", \"B_4\", \"B_5\", \"C_5\"]  # regression: threshold 0.6\n",
    "files = [file[cols_to_include] for file in files]\n",
    "\n",
    "# Test-train split\n",
    "train_files, test_files = test_train_split(files, split=TEST_TRAIN_SPLIT)\n",
    "\n",
    "print(\"Loaded and preprocessed data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SOM_MAP_X_DIMENSION = 10      # 50 as taken from research paper\n",
    "SOM_MAP_Y_DIMENSION = 10     # 50 as taken from research paper\n",
    "SOM_TRAINING_EPOCHS = 10000\n",
    "\n",
    "def train_model(training_data):\n",
    "    model = SOM(x=SOM_MAP_X_DIMENSION, y=SOM_MAP_Y_DIMENSION, input_len=training_data.shape[1], random_seed=RANDOM_SEED)\n",
    "    model.train(training_data, num_iteration=SOM_TRAINING_EPOCHS, verbose=True)\n",
    "    return model\n",
    "\n",
    "models = [train_model(train.to_numpy()) for train in train_files]\n",
    "\n",
    "print(\"SOM models trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOM: Activation on training data (above) and test data (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(8, 2, sharex=True, sharey=True)\n",
    "fig.set_size_inches(6, 24)\n",
    "\n",
    "for i, test_file in enumerate(test_files):\n",
    "    frequencies = models[i].activation_response(train_files[i].to_numpy())\n",
    "    ax[i, 0].pcolor(frequencies.T, cmap='Blues')\n",
    "    ax[i, 0].set_title(f\"Training - Experiment {EXPERIMENT_NAMES[i]}\")\n",
    "\n",
    "    frequencies = models[i].activation_response(test_file.to_numpy())\n",
    "    ax[i, 1].pcolor(frequencies.T, cmap='Oranges')\n",
    "    ax[i, 1].set_title(f\"Test - Experiment {EXPERIMENT_NAMES[i]}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOM: Plot quantization errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "converted_files = [[file[i:i+1].to_numpy() for i in range(len(file) - 1)] for file in files]    # always grab two data points and convert them to numpy (two needed because MiniSom will not evaluate a single one)\n",
    "\n",
    "quantization_errors = [pd.DataFrame([model.quantization_error(data_point)\n",
    "                                     for data_point in converted_files[j]])\n",
    "                       for j, model in enumerate(models)]\n",
    "\n",
    "rolling_medians_som = [qe.rolling(WINDOW).median() for qe in quantization_errors]\n",
    "\n",
    "print(\"Quantization errors computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_all_experiments(rolling_medians_som, EXPERIMENT_NAMES, test_train_split=TEST_TRAIN_SPLIT,\n",
    "                     ols_line=True, savefig=f\"C:/Users/Daniel/Desktop/som_final_qerror.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOM: Anomaly Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_bmu_weights(model, row: pd.Series) -> np.ndarray:\n",
    "    \"\"\"Weights of Best Matching Unit\"\"\"\n",
    "    return model._weights[model.winner(row.to_numpy())]\n",
    "    \n",
    "def most_diverging_feature(x: pd.Series, bmu_weights: np.ndarray) -> str:\n",
    "    return (x - bmu_weights).abs().idxmax()\n",
    "\n",
    "som_anom_loc = [[most_diverging_feature(row, get_bmu_weights(models[i], row))\n",
    "                 for index, row in file.iterrows()]\n",
    "                for i, file in enumerate(files)]\n",
    "\n",
    "colors_som_anom_loc = [[COLORMAP[el] for el in file] for file in som_anom_loc]\n",
    "                           \n",
    "print(\"Computed SOM Anomaly localization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(8, 1)\n",
    "fig.set_size_inches(6, 24)\n",
    "\n",
    "patches = [matplotlib.patches.Patch(color=value, label=key, edgecolor=\"b\") for key, value in COLORMAP.items()]\n",
    "\n",
    "for i, dataset in enumerate(files):\n",
    "    axis = ax[i]\n",
    "    axis.scatter(x=quantization_errors[i].index, y=quantization_errors[i], c=colors_som_anom_loc[i][:-1], s=1)\n",
    "    axis.set_title(f\"Experiment No. {EXPERIMENT_NAMES[i]}\")\n",
    "\n",
    "    # Add test_train_split line\n",
    "    test_train_split_index = int(len(dataset) * TEST_TRAIN_SPLIT)\n",
    "    axis.axvline(x=test_train_split_index, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "    \n",
    "    # Add legend\n",
    "    axis.legend(handles=patches, loc=\"center right\", bbox_to_anchor=(1.22, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"C:/Users/Daniel/Desktop/SOM_final_anom_loc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OneClass SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "SVM_KERNEL = \"rbf\"\n",
    "SVM_NU = 0.01\n",
    "\n",
    "models_svm = [OneClassSVM(kernel=SVM_KERNEL, nu=SVM_NU).fit(train) for train in train_files]\n",
    "predictions_svm = [pd.DataFrame(model.decision_function(files[i])) for i, model in enumerate(models_svm)]\n",
    "\n",
    "rolling_medians = [-1 * pred.rolling(WINDOW).median() for pred in predictions_svm]  # multiply with -1 so that plot has same orientation as SOM plot\n",
    "\n",
    "print(\"OneClassSVM models fitted.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OneClassSVM: Deviation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_all_experiments(rolling_medians, EXPERIMENT_NAMES, TEST_TRAIN_SPLIT,\n",
    "                     ols_line=True, savefig=\"C:/Users/Daniel/Desktop/SVM_final_deviation.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OneClassSVM: Anomaly Localization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_bmu(x: pd.Series, model: OneClassSVM) -> np.ndarray:\n",
    "    support_vectors = model.support_vectors_\n",
    "    distances = np.linalg.norm(support_vectors - x.to_numpy(), axis=1)\n",
    "    min_index = pd.Series(distances).idxmin()\n",
    "\n",
    "    return support_vectors[min_index]\n",
    "\n",
    "def most_diverging_feature(x: pd.Series, model: OneClassSVM) -> str:\n",
    "    bmu = get_bmu(x, model)\n",
    "    return (x - bmu).abs().idxmax()\n",
    "\n",
    "oc_svm_anom_loc = [[most_diverging_feature(row, models_svm[i])\n",
    "                          for index, row in file.iterrows()]\n",
    "                         for i, file in enumerate(files)]\n",
    "\n",
    "colors_oc_svm_anom_loc = [[COLORMAP[el] for el in file] for file in oc_svm_anom_loc]\n",
    "\n",
    "print(\"Computed OneClassSVM Anomaly localization.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(8, 1)\n",
    "fig.set_size_inches(6, 24)\n",
    "\n",
    "patches = [matplotlib.patches.Patch(color=value, label=key, edgecolor=\"b\") for key, value in COLORMAP.items()]\n",
    "\n",
    "for i, dataset in enumerate(files):\n",
    "    axis = ax[i]\n",
    "    axis.scatter(x=rolling_medians[i].index, y=rolling_medians[i], c=colors_oc_svm_anom_loc[i], s=1)\n",
    "    axis.set_title(f\"Experiment No. {EXPERIMENT_NAMES[i]}\")\n",
    "\n",
    "    # Add test_train_split line\n",
    "    test_train_split_index = int(len(dataset) * TEST_TRAIN_SPLIT)\n",
    "    axis.axvline(x=test_train_split_index, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    # Add legend\n",
    "    axis.legend(handles=patches, loc=\"center right\", bbox_to_anchor=(1.22, 0.5))\n",
    "    axis.set_ylim(bottom=0.0)   # plot only outliers (decision_function <= 0 umgedreht durch -1 oben)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"C:/Users/Daniel/Desktop/SVM_final_anom_loc.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Anomaly localization as stackplot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "def set_others_na(l, value: str) -> list:\n",
    "    return [(el if el == value else np.NaN) for el in l]\n",
    "\n",
    "WINDOW = 300\n",
    "\n",
    "occurrences_all = [\n",
    "    [\n",
    "        pd.Series(set_others_na(experiment, \"L_2\")).rolling(WINDOW).count() / WINDOW,\n",
    "        pd.Series(set_others_na(experiment, \"L_6\")).rolling(WINDOW).count() / WINDOW,\n",
    "        pd.Series(set_others_na(experiment, \"L_8\")).rolling(WINDOW).count() / WINDOW,\n",
    "        pd.Series(set_others_na(experiment, \"L_10\")).rolling(WINDOW).count() / WINDOW,\n",
    "        pd.Series(set_others_na(experiment, \"A_5\")).rolling(WINDOW).count() / WINDOW,\n",
    "        pd.Series(set_others_na(experiment, \"B_4\")).rolling(WINDOW).count() / WINDOW,\n",
    "        pd.Series(set_others_na(experiment, \"B_5\")).rolling(WINDOW).count() / WINDOW,\n",
    "        pd.Series(set_others_na(experiment, \"C_5\")).rolling(WINDOW).count() / WINDOW\n",
    "    ] for experiment in oc_svm_anom_loc\n",
    "]\n",
    "\n",
    "print(\"Occurrences computed.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(8, 1)\n",
    "fig.set_size_inches(6, 24)\n",
    "\n",
    "for i, experiment in enumerate(occurrences_all):\n",
    "    axis = ax[i]\n",
    "    axis.set_title(f\"Experiment No. {EXPERIMENT_NAMES[i]}\")\n",
    "\n",
    "    index = list(range(len(experiment[0])))\n",
    "    axis.stackplot(index, *occurrences_all[i],\n",
    "                   labels=COLORMAP.keys(),\n",
    "                   colors=COLORMAP.values())\n",
    "\n",
    "    # Add test_train_split line\n",
    "    test_train_split_index = int(len(experiment[0]) * TEST_TRAIN_SPLIT)\n",
    "    axis.axvline(x=test_train_split_index, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    axis.legend(loc=\"center right\", bbox_to_anchor=(1.22, 0.5))\n",
    "    axis.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"C:/Users/Daniel/Desktop/SVM_final_anom_loc_stackplot.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}